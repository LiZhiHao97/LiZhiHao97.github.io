---
title: 论文速读：MoCoGAN Decomposing Motion and Content for Video
date: 2020-12-21 21:26:14
tags:
  - 深度学习
  - 视频生成
  - GAN
  - Unconditioned Video Generation
---

## 1. Introduction

视频中的视觉信号可以分为内容和运动。内容指定视频中的对象，运动描述它们的动态。在此基础上，本文提出了基于运动和内容分解的生成性对抗网络(MoCoGAN)的视频生成框架。

> 代码地址：https://github.com/sergeytulyakov/mocogan.

### 1.1 Related Work

- 使用循环机制来生成视频片段中帧的运动嵌入（motion）。
- 使用卷积神经网络生成图像（content）。

### 1.2 Contribution

1. 我们提出了一种新的用于无条件视频生成的GAN框架，将**噪声向量（noise）**映射到视频中。
2. 我们展示了所提出的框架提供了一种在视频生成中控制**内容**和**运动**的方法，这是现有视频生成框架所缺乏的。
3. 为了验证算法的有效性，我们在基准数据集上进行了大量的实验验证，并与目前最先进的视频生成算法**VGAN**和**TGAN**进行了定量和主观比较

## 2. MoCoGAN Framework



**MoCoGAN**由四个子网络组成，即循环神经网络$$ R_{M} $$、图像生成器$$ G_{I} $$、图像鉴别器$$ D_{I} $$和视频鉴别器$$ D_{V} $$

![1](https://i.loli.net/2020/12/21/m5Va9dhrUixQyI6.png)

1. 使用高斯分布初始化$$ Z_{C} $$对内容子空间建模，由于在剪辑的短视频中内容基本保持不变，所以我们使用相同的$$ Z_{C} $$来生成不同帧。
2. 视频中的运动由动作子空间$$ Z_{M} $$建模。用于作为$$ G_{I} $$输入的向量序列**Z**表示

![2](https://i.loli.net/2020/12/21/8rB9gz1fp6UKXLq.png)
> 其中$$ z_{C} $$ ∈  $$ Z_{C} $$以及$$ z_{M}^{(k)} $$ ∈ $$ Z_{M} $$都各有k个，因为$$ Z_{M} $$中并不是所有的路径都对应于路径上合理的运动，我们需要学会生成有效的路径，通过循环神经网络对路径生成过程进行建模。

3. $$ R_{M} $$即为一个循环神经网络，在每一个时间步**k**，都会输入一个服从高斯分布的$$ \epsilon^{(k)} $$,以及输出一个$$ z_{M}^{(k)} $$。假设$$ R_{M} $$(k)为时刻k的输出，则$$ z_{M}^{(k)} $$ = $$ R_{M} $$(k)。本论文中，使用**GRU**作为$$ R_{M} $$的实现。
4. $$ G_{I} $$将向量序列**Z**映射为向量序列$$ \tilde{v} $$，其中$$ \tilde{v} $$ = [$$ \tilde{x}^(1) $$, ..., $$ \tilde{x}^(k) $$]
5. $$ D_{I} $$和$$ D_{V} $$分别用于鉴别单个图像和视频序列。$$ D_{V} $$获取固定长度的视频片段，例如**T**帧，**T**为一个超参数，论文中设为**16**。**T**可以小于所生成的视频长度**K**。长度为K的视频可以以滑动窗口的方式被分成**K-T+1**个片段，并且每个片段都可以被馈送到$$ D_{V} $$中。
6. $$ D_{V} $$也用来评估所生成的动作。因为$$ G_{I} $$没有运动的概念，因此对于运动的损失将会直接指向循环神经网络$$ R_{M} $$。为了骗过$$ D_{V} $$，生成具有较为真实动作的视频，$$ R_{M} $$必须学会将输入$$ \epsilon^{(k)} $$映射为动作系列**Z**

## 3. Categorical Dynamics
视频中的动态通常是绝对的(例如，离散的动作类别：行走、跑步、跳跃等)。为了对这个分类信号建模，我们用一个分类随机变量$$Z_{A}$$来增加RM的输入，其中每个实现都是一个**one-hot**向量。我们保持其不变，因为短视频中动作类别保持不变，因此$$ R_{M} $$的输入如下图所示。

![3](https://i.loli.net/2020/12/21/8Tklze5RjWM2JqL.png)

## 4. Conclusion
以上。